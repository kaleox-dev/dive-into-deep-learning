intro
- how can we be sure we discovered a general pattern and not simply memorized our data?
- overfitting: fiting closer to our training data than the underlying distribution
- regularization: combatting overfitting

summary
- use validation sets for model selection
- more complex models often require more data
- complexity: number of params, range of values params can taken
- more data almost always leads to better generalization
- this is all predicated on the IID assumption 

key term:
IID: data points are unrelated to each other (indepedent), and drawn from the same underlying probability distribution

3.6.1. training error and generalization error
- standard supervised learning: training data and test data are drawn independently from identitical distributions
- why should we believe predictions on test data from distr P(X, Y) should tell us how to make predictions on 
test data generated by a different distribution Q(X, Y)
- training error: statistic calculated on the training dataset
- generalization error: expectation taken with respect to the underlying distribution
- training error is the average of the losses
- generalization error is the double integral with respect to the loss in dx and dy
- we can only estimate the generalization error
- central question: when should we expect our training error to be close to the population error

3.6.1.1. model complexity
- weight decay: regularizaton technique
- when a model is capable of fitting arbitrary labels, low training error does not necessarily 
imply low generalization error, nor high generalization error

3.6.2. underfitting or overfitting?
- if the model is unable to reduce the training error, that could mean the model is too simple to capture the pattern
- since the generalization gap between training and generalization errors is small, we might get away with more complex model 
(underfitting)
- when training error is significantly lower than our validation error, indicates severe overfitting

3.6.2.1 polynomial curve fitting
- example: traning data consisting of single feature x and a corresponding real=valued label y (single var regression)
- try to find some polynomial of degree d 
- our features are given by the powers of x [1, x, x^2, ..., x^d]
- weights are the coefficients of each power of x 
- the bias is w_0
- squared error is our loss function
- training error(higher order poly) <= training error(lower order poly)
- whenever each data example has a distinct valueof x, a poly func with degree equal to the number of 
data examples can fit the training set perfectly

3.6.2.2. dataset size
- the fewer samples in training dataset, the more likely to encounter overfitting
- in general, more data never hurts
- for many tasks, DL only outperforms linear models when many thousands of training examples are available

3.6.3. model selection
- do not use test data in the model selection, or else overfit test data
- split data three ways (training, test, validation)

3.6.3.1. cross-validation
- when training data is not snough, we cant hold out proper validation set
- K-folder cross-validation
- split data into K non-overlapping subsets
- model training and validation are executed K times, each time training on K-1 subsets and then 
validating on a different subset (the one not used for training in that round)
- training and validation errors are estimated by averaging over the results from K experiments

3.6.5. exercises
1. when the number of data points is equal to the degree of the polynomial
2. pass
3. yes. zero generalization error when the model is perfect? or validation sets the same as training set
4. because you have to compute K times instead of once
5. pass
6. pass
7. show them that there is a positive correlation between amount of data and model performance
when we decrease the data, the model does poorly. 
the more we decrease the data, the model does worse and worse
this leads to the inference that as we increase the data, the model does better and better